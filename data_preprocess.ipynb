{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据并预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['弹幕', '类别'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>弹幕</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>你代言都和他接同一家？？？？</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《我   很   内   向》</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不管是不是笋 秀芬看到即爽到</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>你好爱他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>西蒙他老婆的战歌</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>啊啊啊鹿晗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>啊啊啊啊啊啊啊啊，鹿鹿子</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>就尼玛离谱，鹿晗怎么能这么好看</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>完颜团不是吹的</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>啊啊啊啊啊啊伯贤也太嫩了！！！！可爱死啦！！！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           弹幕  类别\n",
       "0              你代言都和他接同一家？？？？   2\n",
       "1             《我   很   内   向》   1\n",
       "2              不管是不是笋 秀芬看到即爽到   0\n",
       "3                        你好爱他   0\n",
       "5                    西蒙他老婆的战歌   0\n",
       "...                       ...  ..\n",
       "1190                    啊啊啊鹿晗   0\n",
       "1192             啊啊啊啊啊啊啊啊，鹿鹿子   3\n",
       "1194          就尼玛离谱，鹿晗怎么能这么好看   3\n",
       "1195                  完颜团不是吹的   2\n",
       "1196  啊啊啊啊啊啊伯贤也太嫩了！！！！可爱死啦！！！   0\n",
       "\n",
       "[48908 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = r\"./data/source_data/\"\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "df_all = pd.DataFrame({\"弹幕\":[],\"类别\":[]})\n",
    "print(df_all.columns)\n",
    "df = 0\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f) \n",
    "    df = df.iloc[:,-2:]\n",
    "    df.columns = [\"弹幕\", \"类别\"]   \n",
    "    df_all = pd.concat([df_all, df],axis=0)\n",
    "    # print(df_all[\"弹幕\"][0])  \n",
    "\n",
    "df_all.dropna(inplace=True) \n",
    "df_all[\"类别\"] = df_all[\"类别\"].apply(lambda x: int(x))\n",
    "df_all.drop_duplicates(subset=[\"弹幕\",\"类别\"],keep='first',inplace=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [\"高兴\",\"难过\",\"愤怒\",\"惊讶\",\"负样本\"]\n",
    "# for i in range(len(labels)):\n",
    "#     df_all[\"类别\"][df_all[\"类别\"]==i] = labels[i]\n",
    "# df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    加载本地字典：\n",
    "    【1】自定义字典\n",
    "    【2】停用词字典\n",
    "\"\"\"\n",
    "local_dic_name = './data/userdict.txt'\n",
    "local_stopwords_name = './data/stopwords_dic.txt'\n",
    "jieba.load_userdict(local_dic_name)\n",
    "jieba.load_userdict(local_stopwords_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x = []\n",
    "dataset_y = []\n",
    "for i in range(len(df_all)):\n",
    "    str_t = str(df_all.iloc[i][\"弹幕\"])\n",
    "    label = int(df_all.iloc[i][\"类别\"])\n",
    "    word_list = jieba.lcut(str_t)\n",
    "    dataset_x.append(word_list)\n",
    "    dataset_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['你', '代言', '都', '和', '他', '接同', '一家', '？', '？', '？', '？'],\n",
       "  ['《', '我', ' ', ' ', ' ', '很', ' ', ' ', ' ', '内', ' ', ' ', ' ', '向', '》'],\n",
       "  ['不管', '是不是', '笋', ' ', '秀芬', '看到', '即爽', '到'],\n",
       "  ['你好', '爱', '他'],\n",
       "  ['西蒙', '他', '老婆', '的', '战歌']],\n",
       " [2, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x[:5],dataset_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除停用词(暂时不使用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "#     函数功能：创建停用词list\n",
    "#     参数：\n",
    "#         filepath：停用词典地址\n",
    "#     返回：\n",
    "#          停用词list\n",
    "# \"\"\"\n",
    "# def stopwordslist(local_stopwords_name):\n",
    "#     stopwords = [line.strip() for line in open(local_stopwords_name, 'r', encoding='utf-8').readlines()]\n",
    "#     return stopwords\n",
    "\n",
    "# stopwords = stopwordslist(local_stopwords_name)\n",
    "\n",
    "# def word_filter(result):\n",
    "#     stopwords = stopwordslist(local_stopwords_name)\n",
    "#     body = ''\n",
    "#     for w in result:\n",
    "#         if w.flag != 'x' and w.flag != 'r' and w.flag != 'ul' \\\n",
    "#                 and w.flag != 'uj' and w.flag != 'y' and w.flag != 'q'\\\n",
    "#                 and w.flag != 'd' and w.flag != 'm' and w.flag != 'eng':\n",
    "#             if w.word not in stopwords:\n",
    "#                 body += w.word + '\\n'\n",
    "#     # 提取关键词\n",
    "#     tag = jieba.analyse.extract_tags(body, 5)\n",
    "#     # print(tag)\n",
    "#     # 生成关键词比重词典\n",
    "#     # key = jieba.analyse.textrank(body, topK=100, withWeight=True)\n",
    "#     # keywords = dict()\n",
    "#     # for i in key:\n",
    "#     #    keywords[i[0]] = i[1]\n",
    "#     # print(keywords)\n",
    "#     return body\n",
    "\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     dataset[i][0] = word_filter(dataset[i][0])\n",
    "\n",
    "# dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据词频OneHot编码（暂时不用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 将原始训练和测试文本转化为特征向量\n",
    "# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "# # count_vec=CountVectorizer() #创建词袋数据结构\n",
    "# count_vec=TfidfVectorizer() #根据词频-逆文档频率\n",
    "# dataset_count_x = count_vec.fit_transform(dataset_x) \n",
    "# dataset_count_x= dataset_count_x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "# 训练word to vector 的 word embedding\n",
    "model = Word2Vec(dataset_x,     # 上文处理过的全部语料\n",
    "                 min_count=1,  # 词频阈值 词出现的频率 小于这个频率的词 将不予保存\n",
    "                 workers=12, # worker是线程数\n",
    "                 window=5  # 窗口大小 表示当前词与预测词在一个句子中的最大距离是多少\n",
    "                 )\n",
    "model.save('./models/Word2vec_v1')  # 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load(\"./models/Word2vec_v1\")#加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01764946, -0.00614291, -0.27418613, -0.2186066 ,  0.06597638,\n",
       "       -0.19655891,  0.01743046,  0.20372109,  0.31656262,  0.03259294,\n",
       "        0.00404844, -0.13648203, -0.12413061,  0.18599075,  0.02434096,\n",
       "       -0.00740953, -0.17991793, -0.00444531,  0.1427604 ,  0.1395671 ,\n",
       "        0.04938675, -0.05333138,  0.06338228,  0.14239158, -0.02861923,\n",
       "        0.06134237,  0.14270352,  0.08790725, -0.03099502,  0.09653117,\n",
       "        0.05480843, -0.1184715 ,  0.00252008,  0.02443198, -0.16614774,\n",
       "       -0.0884072 , -0.24593821, -0.05610939,  0.03413791,  0.02590739,\n",
       "        0.09474245, -0.1458126 ,  0.01417803,  0.12829913,  0.01174895,\n",
       "        0.12766293,  0.23352446,  0.12166523, -0.06767468,  0.11412152,\n",
       "        0.09006815, -0.08509684,  0.17986457, -0.11777854, -0.11041456,\n",
       "       -0.08263613,  0.17084052, -0.14683399, -0.02112032, -0.08350345,\n",
       "       -0.16239052,  0.06931621,  0.22068495, -0.1578224 , -0.16823825,\n",
       "        0.1736363 ,  0.04730841,  0.06706356,  0.02345077,  0.09263737,\n",
       "        0.0033448 ,  0.02207851,  0.2393472 , -0.00796847,  0.1012794 ,\n",
       "        0.25404337, -0.15185304, -0.2092999 , -0.10238035, -0.04198622,\n",
       "       -0.33824328,  0.09174774,  0.12209141,  0.07351775,  0.06164327,\n",
       "       -0.10861819, -0.16399893,  0.2846328 ,  0.2770344 , -0.06934463,\n",
       "        0.11951667,  0.01836435,  0.12831736,  0.14912459, -0.10828809,\n",
       "       -0.05090778,  0.180853  ,  0.20735636,  0.0397816 ,  0.14556524],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector(\"狂喜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vector_x = []\n",
    "for sentence in dataset_x:\n",
    "    vector_x = []\n",
    "    for word in sentence:\n",
    "        vector_x.append(model.wv.get_vector(word))\n",
    "    dataset_vector_x.append(vector_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 15)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_vector_x[0]),len(dataset_vector_x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48908, 5)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "dataset_y = np.array(dataset_y).reshape(-1,1)\n",
    "enc.fit(dataset_y)\n",
    "dataset_onehot_y = enc.transform(dataset_y).toarray()\n",
    "dataset_onehot_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset_vector_x,\"dataset/dataset_x\")\n",
    "torch.save(dataset_onehot_y,\"dataset/dataset_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6429919fe5eee10fa3db4376c75d0431aac4ee64633f3fde6de3e71a7b7c5c41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
